---
title: 'moltenPPCA'
author: "Ko ABE"
date: "3/22/2022 (Last update: 3/25/2022)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{moltenPPCA}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE, fig.width = 14, fig.height = 10, fig.align = "center"}
knitr::opts_chunk$set(echo = TRUE)
```

## Motivation

Principal component analysis (PCA) is widely used in exploratory data analysis. 

Commonly, PCA and probabilistic PCA (PPCA) find the matrix $Z$ and $W$ such that $Y \approx ZW$ with given data matrix $Y$.

However, If the data contains some side-information (such as, time, age, sex, or treatment), matrix format will probably failed to represent data structure. 

On the other hand, *tidy* data format are flexible. Now, we introduce *tidy* version of PPCA.

## Model (PCA)

Now, We consider the probabilistic model for observed variable $y_n$ as following; 
$$
y_n \sim \mathcal{N}\left(\sum_{l=1}^L \prod_{d=1}^Dv_{dl}^{x_{nd}}, \lambda^{-1}\right),
$$
where $\mathcal{N}(\mu,\sigma^2)$ is normal distribution with mean $\mu$ and variance $\sigma^2$. The precision $\lambda$ is user-tuning parameter.

Next, we set normal prior distribution for unobserved variable $v_{dl}$;
$$
v_{dl} \sim \mathcal{N}(0,\tau^{-1}).
$$

### Example: matrix factorization

We assume that we obtain two response from an experiment with the two subjects. Let matrix $Y=(y_{nk})$ be the result of this experiment and its $(n,k)$-element be $k$-th response from subject $n$. This data-set $\mathcal{D}$ can be expressed as following:
$$
\mathcal{D} = \{ (1, 1, y_{1}), (2, 1, y_{2}), (1, 2, y_{3}), (2,2, y_{4})\},
$$
where $i$ of triple $(i,j,h)$ represent subject, and $j$ is item dummy variable. 
In this notation, the indices of the $y_{nk}$ can be regarded as explanatory variables as following:
$$
\boldsymbol{x}_{1j} = \{1,0\}, \quad \boldsymbol{x}_{2j} = \{0,1\}
$$
Under the assumption of proposal model, $y_{nk}$ can be approximated as:
$$
y_h ~ (\, =y_{nk}) \approx \sum_{l=1}^L \prod_{d=1}^D v_{dl}^{x_{hd}}.
$$

This equation is equivalent to the matrix factorization:
$$
y_{nk} = y_{h} \approx \sum_{l=1}^L z_{nl}w_{lk}.
$$
The proposed model contains matrix factorization as special case.

### Missing and duplicate observation

We consider the situation where there are subjects that are observed at the first time but not at the second time.
If such data is held as a three-dimensional array, there will be a missing value at the second time point.

The proposed model can naturally treat such missing data. If the subjects does not exist at the second time point, there is no corresponding row in the data but there is no problem for estimation procedure.

Similarly, if you have duplicate observation from same condition, you may not to remake the array which contains the data but the corresponding rows be added.

## Estimation

Under the model assumption and using mean-field approximation, we get following variational approximated posterior distribution; 
$$
q(v_{dl})= \mathcal{N}(\hat \mu_{dl},\hat \sigma_{dl}), 
$$
where $\hat \mu_{dl}$ is defined by,
$$
\hat \mu_{dl} =\frac{\sum_{n=1}^N x_{nd}\prod_{d' \neq d} \langle v_{d'l} \rangle ^{x_{nd'}} \left[y_n -\sum_{l'\neq l}\prod_{j=1}  \langle v_{jl'} \rangle ^{x_{nj}}  \right]}{\tau/\lambda + \left( \sum_{n=1}^N x_{nd}\prod_{d\neq d'} \langle v_{d'l}^{2} \rangle ^{x_{nd'}} \right)},
$$
and $\hat \sigma_{dl}$ is
$$
{\hat{\sigma}}^2 =\left(\tau + \lambda \left\{\sum_{n=1}^N x_{nd}\prod_{d' \neq d}  \langle v_{d'l}^2 \rangle ^{x_{nd'}}\right\} \right)^{-1},
$$
where $\langle v_{dl} \rangle$ is expectation of $v_{dl}$ under the all of variational posterior distributions $q(v_{dl})$ except itself.


## Workflow (iris)

Run `mPPCA_vb` and check convergence:
```{r run}
library(moltenPPCA)
library(tidyr)
library(dplyr)
library(ggplot2)
iris_g <- mutate(iris, id=factor(row_number())) %>% 
  pivot_longer(!(id|Species), names_to = "key")

set.seed(753)
out <- mPPCA_vb(value~id+key-1,data=iris_g,L=2, iter=200)
qplot(1:200,out$logloss, geom = "line")+
  labs(x="iter", y="log-loss")+
  theme_minimal(16)
```
Plot components:
```{r plot}
df <- data.frame(out$mean[out$vargroup=="id",], species=iris$Species)

ggplot(df, aes(X1,X2,colour=species))+
  geom_point(alpha=0.7, size=2)+
  theme_minimal(16)
```

As you can see, setosa and virginica are very different. On the other hand virginica and virgicalor are relatively similar.

## Model (NMF)

Now, we consider the case of handling to the count data. 
In this case, we might be not enjoyable to the predictor which generate negative number.

If you choose fitting the model to data using mean squared error, it is equivalent to set the observation model as normal distribution.

Poisson loss is one of the options in the such case.

We consider the probabilistic model for observed variable $y_n$ as following;
$$
y_n \sim \mathcal{P}\left(\sum_{l=1}^L \prod_{d=1}^Dv_{dl}^{x_{nd}}\right),
$$
where $\mathcal{P}(\lambda)$ is Poison distribution with mean $\lambda$ and variance $\lambda$.

This model is equivalent to following data generating process;
$$
y_n = \sum_{l=1}^L u_{nl}, \quad u_{nl} \sim \mathcal{P}\left(\prod_{d=1}^Dv_{dl}^{x_{nd}}\right),
$$

We set gamma prior distribution with shape parameter $a$, and rate $b$ for unobserved variable $v_{dl}$;
$$
v_{dl} \sim \mathcal{G}(a,b).
$$
The mean-field posterior $q(v_{dl})$ is gamma distribution with shape parameter,
$$
\hat a_{dl} = \sum_{n=1}^{N} x_{nd} \langle u_{nl}\rangle + a
$$
and rate parameter,
$$
\hat b_{dl} = \sum_{n=1}^{N} x_{n,d} \left( \prod_{d'\neq d} \langle \lambda_{d'l} \rangle^{x_{nd'}} \right) + b.
$$
The updates for $\langle u_{nl} \rangle$ are given by $\langle u_{nl} \rangle = y_{n} r_{nl}$ where $r_{nl}$ is defined as, 
$$
r_{nl} = \frac{\exp( x_{nd} \langle \log v_{dl} \rangle)}{\sum_{l=1}^{L} \exp( x_{nd} \langle \log v_{dl} \rangle)}.
$$

## Workflow (Titanic)

Run `mNMF_vb`;
```{r titanic}
Titanicdf <- as.data.frame(Titanic) %>% 
  mutate(Class=factor(Class,levels=c("3rd","2nd","1st","Crew")))

set.seed(794)
out2 <- mNMF_vb(Freq~Survived+Class+Sex+Age-1, data=Titanicdf, L=2, iter=500)
```

To check convergence:
```{r titanic2}
qplot(1:length(out2$ELBO),out2$ELBO, geom = "line")+
  labs(x="iter",y="ELBO")+
  theme_minimal(16)
```


The goodness-of-fit:
```{r titanic3}
V <- out2$shape/out2$rate
yhat <- rowSums(prod_mPPCA(~Survived+Class+Sex+Age-1, data=Titanicdf,V))
qplot(yhat, Titanicdf$Freq, alpha=I(0.5),size=I(3))+
  labs(x="fit",y="obs")+
  geom_abline(intercept=0,slope=1,linetype=2)+
  theme_minimal(16)
```

To show parameters $v_{dl}$:
```{r titanic4}
Vdf <- data.frame(V,variable=rownames(V),
                  facet_dummy=out2$vargroup) %>% 
  pivot_longer(!(variable|facet_dummy), names_to = "component", 
               names_transform = list(component = readr::parse_number)) %>% 
  mutate(component=factor(component))

p1 <- ggplot(Vdf, aes(y=variable, x=value, fill=component))+
  geom_col(colour="gray20")+
  facet_grid(facet_dummy~.,scales="free_y",space = "free")+
  theme_classic(16)

p2 <- ggplot(Vdf, aes(y=variable, x=value, fill=component))+
  geom_col(colour="gray20", position = "fill")+
  facet_grid(facet_dummy~.,scales="free_y",space = "free")+
  scale_x_continuous(labels=scales::percent)+
  theme_classic(16)

print(p1)
print(p2)
```

These plot show co-occurrence relationships between variables. 
You can see that many of survivors of Titanic are women, because of `SurvivalYes`  and `SexFemale` have large proportion `component 1`.

## Model (probit)

Finally, let's take it up the case of binary outcomes:
$$
y_n = H(z_n), \quad z_n \sim \mathcal{N}\left(\sum_{l=1}^L \prod_{d=1}^Dv_{dl}^{x_{nd}}, \lambda^{-1}\right),
$$
where $H(x)$ is Heaviside step function:
$$
H(x)=\begin{cases}1,&x>0\\0, &x\leq 0\end{cases}.
$$

In this model, estimation procedure can be derive naturally from above mentioned normal model using Monte-Carlo sample of latent variables $z_n$.

## Workflow (...)

(I'm looking for a good example.)